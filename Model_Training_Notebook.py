# -*- coding: utf-8 -*-
"""Model_Training_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FcUK7q1ywfnby9yeqZfvLu-pZ2kbGxtC
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings(action='ignore')
import joblib
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import os
from datetime import datetime
from sklearn.metrics import ConfusionMatrixDisplay

data = pd.read_csv('cumulative.csv')
data.head(5)

def preprocess_inputs(df):
    df = df.copy()

    # Drop unused columns
    df = df.drop(['rowid', 'kepid', 'kepoi_name', 'kepler_name',
                  'koi_pdisposition', 'koi_score', 'koi_tce_delivname',
                 'koi_fpflag_nt','koi_fpflag_ss','koi_fpflag_co','koi_fpflag_ec',
                 'koi_tce_plnt_num'], axis=1) # the last 4 columns are removed because after scaling they are almost 0

    # Limit target values to CANDIDATE and CONFIRMED
    false_positive_rows = df.query("koi_disposition == 'FALSE POSITIVE'").index
    df = df.drop(false_positive_rows, axis=0).reset_index(drop=True)

    # Drop columns with all missing values
    df = df.drop(['koi_teq_err1', 'koi_teq_err2'], axis=1)

    # Identify numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'koi_disposition' in numeric_cols:
        numeric_cols.remove('koi_disposition')

    # Handle outliers using IQR
    for col in numeric_cols:
        if df[col].notna().sum() > 0:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR  # Using 3*IQR for less aggressive outlier removal
            upper_bound = Q3 + 3 * IQR
            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

    # Fill missing values with median as its more robust than mean
    for column in df.columns[df.isna().sum() > 0]:
        df[column] = df[column].fillna(df[column].median())

    # Feature engineering - create interaction features
    # Transit depth to stellar radius ratio
    if 'koi_depth' in df.columns and 'koi_srad' in df.columns:
        df['depth_to_srad'] = df['koi_depth'] / (df['koi_srad'] + 1e-10)

    # Planet-star radius ratio
    if 'koi_prad' in df.columns and 'koi_srad' in df.columns:
        df['prad_to_srad_ratio'] = df['koi_prad'] / (df['koi_srad'] + 1e-10)

    # Orbital period to impact parameter ratio
    if 'koi_period' in df.columns and 'koi_impact' in df.columns:
        df['period_to_impact'] = df['koi_period'] / (df['koi_impact'] + 1e-10)

    # Insolation flux feature
    if 'koi_insol' in df.columns:
        df['log_insol'] = np.log1p(df['koi_insol'])

    # Signal-to-noise ratio features
    if 'koi_model_snr' in df.columns:
        df['log_snr'] = np.log1p(df['koi_model_snr'])

    # Remove highly correlated features (> 0.95 correlation) to eliminate multicollinearity
    numeric_df = df.select_dtypes(include=[np.number])
    corr_matrix = numeric_df.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
    df = df.drop(to_drop, axis=1)

    # Split df into X and y
    y = df['koi_disposition']
    X = df.drop('koi_disposition', axis=1)

    # Train-test split with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, train_size=0.7, shuffle=True, random_state=1, stratify=y
    )

    # Normalizing features
    scaler = RobustScaler()
    scaler.fit(X_train)
    joblib.dump(scaler, 'scaler.pkl')
    X_train = pd.DataFrame(scaler.transform(X_train),
                          index=X_train.index, columns=X_train.columns)
    X_test = pd.DataFrame(scaler.transform(X_test),
                         index=X_test.index, columns=X_test.columns)

    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = preprocess_inputs(data)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")
print(f"Class distribution in training: {y_train.value_counts().to_dict()}")

min_max_df = pd.DataFrame({
    "min": X_train.min(),
    "max": X_train.max()
})

print(min_max_df)

# Models defination
models = {
    "Logistic Regression": LogisticRegression(),
    "      Decision Tree": DecisionTreeClassifier(),
    "      Random Forest": RandomForestClassifier(),
    "  Gradient Boosting": GradientBoostingClassifier(),
    "           LightGBM": LGBMClassifier(),
    "           CatBoost": CatBoostClassifier(verbose=0),
    "     SGD Classifier": SGDClassifier(loss="log_loss"),
    " Naive Bayes (Gauss)": GaussianNB(),
    " Naive Bayes (Bern)": BernoulliNB(),
    "k-Nearest Neighbors": KNeighborsClassifier(),
    "                 LDA": LinearDiscriminantAnalysis(),
    "                 QDA": QuadraticDiscriminantAnalysis(),
    " Neural Net (MLP)": MLPClassifier(max_iter=500)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    print(f"{name} trained.")
    filename = name.strip().replace(" ", "_").lower() + ".pkl"
    joblib.dump(model, filename)
    print(f"Saved {name} to {filename}")

# Evaluation
results = []
plt.figure(figsize=(10, 8))

for name, model in models.items():
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Use sklearn's built-in metrics
    acc = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, pos_label='CONFIRMED', zero_division=0)
    recall = recall_score(y_test, y_pred, pos_label='CONFIRMED', zero_division=0)
    f1 = f1_score(y_test, y_pred, pos_label='CONFIRMED', zero_division=0)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    results.append({
        "Model": name,
        "Accuracy": acc,
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1,
        "ROC-AUC": roc_auc
    })

    fpr, tpr, _ = roc_curve(y_test, y_pred_proba, pos_label='CONFIRMED')
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc:.3f})")

plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves - Improved Preprocessing")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

df_results = pd.DataFrame(results).sort_values('ROC-AUC', ascending=False)
print("\n" + "="*80)
print("MODEL PERFORMANCE COMPARISON")
print("="*80)
print(df_results.to_string(index=False))
print("="*80)

# Save results to CSV
models_dir = os.getcwd()
results_path = os.path.join(models_dir, "model_results.csv")
df_results.to_csv(results_path, index=False)

# Save a README file with usage instructions
readme_path = os.path.join(models_dir, "README.txt")
with open(readme_path, 'w') as f:
    f.write("Exoplanet Classification Models\n")
    f.write("="*50 + "\n\n")
    f.write(f"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Training Samples: {len(X_train)}\n")
    f.write(f"Test Samples: {len(X_test)}\n")
    f.write(f"Features: {X_train.shape[1]}\n\n")
    f.write("How to Load and Use Models:\n")
    f.write("-"*50 + "\n")
    f.write("import joblib\n")
    f.write("import pandas as pd\n\n")
    f.write("# Load a model\n")
    f.write("model = joblib.load('random_forest.pkl')\n\n")
    f.write("# Load the scaler\n")
    f.write("scaler = joblib.load('scaler.pkl')\n\n")
    f.write("# Prepare your data (must have same features as training)\n")
    f.write("# X_new = pd.DataFrame(...)\n")
    f.write("# X_new_scaled = scaler.transform(X_new)\n\n")
    f.write("# Make predictions\n")
    f.write("# predictions = model.predict(X_new_scaled)\n")
    f.write("# probabilities = model.predict_proba(X_new_scaled)\n\n")
    f.write("Model Performance Summary:\n")
    f.write("-"*50 + "\n")
    f.write(df_results.to_string(index=False))


# Find and display best model
best_model = df_results.iloc[0]
print(f"\n{'='*80}")
print(f"BEST MODEL: {best_model['Model']}")
print(f"ROC-AUC: {best_model['ROC-AUC']:.4f}")
print(f"F1 Score: {best_model['F1 Score']:.4f}")
print(f"{'='*80}")

from sklearn.model_selection import cross_val_score

best_clf = models[best_model["Model"]]
cv_scores = cross_val_score(best_clf, X_train, y_train, cv=5, scoring="roc_auc")

print("Cross-Validation ROC-AUC scores:", cv_scores)
print("Mean ROC-AUC:", cv_scores.mean())
print("Std deviation:", cv_scores.std())

from sklearn.model_selection import learning_curve

# === Learning Curve for Best Model ===
train_sizes, train_scores, val_scores = learning_curve(
    best_clf,
    X_train,
    y_train,
    cv=5,
    scoring="roc_auc",
    n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)  # 10 points from 10% to 100% of data
)

# Compute means and stds
train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

# Print values for inspection / sharing
print("\n" + "="*80)
print(f"Learning Curve - {best_model_name}")
print("="*80)
for size, tr_m, tr_s, va_m, va_s in zip(train_sizes, train_mean, train_std, val_mean, val_std):
    print(f"Train size: {size:5d} | "
          f"Train ROC-AUC: {tr_m:.4f} ± {tr_s:.4f} | "
          f"Val ROC-AUC: {va_m:.4f} ± {va_s:.4f}")
print("="*80)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, 'o-', label="Training score", color="blue")
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color="blue")

plt.plot(train_sizes, val_mean, 'o-', label="Validation score", color="green")
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color="green")

plt.xlabel("Training Examples")
plt.ylabel("ROC-AUC")
plt.title(f"Learning Curve - {best_model_name}")
plt.legend(loc="best")
plt.grid(True)
plt.show()





